{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcc94ab2",
   "metadata": {},
   "source": [
    "# Task 1 Text Classification\n",
    "## Dataset: Fetch 20 Newsgroups (same as in class work)​\n",
    "\n",
    "## Algorithms: Multinomial Naïve Bayes, Logistic Regression, Support Vector Machines, Decision Trees​\n",
    "\n",
    "## Feature Extractors: CountVectorizer, Word2Vec, Doc2Vec and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4069e0",
   "metadata": {},
   "source": [
    "### Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eca07fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import logging\n",
    "import re\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c22cc4",
   "metadata": {},
   "source": [
    "### Choose a few categories fro the entire 20 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2097b6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some categories from the training set\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47e696f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:\n",
      "['alt.atheism', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f313ca78",
   "metadata": {},
   "source": [
    "### Fetch documents for these 2 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56f7af48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "857 documents\n",
      "2 categories\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = fetch_20newsgroups(subset='train', categories=categories)\n",
    "print(f\"{len(data.filenames)} documents\")\n",
    "print(f\"{len(data.target_names)} categories\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15c6694d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), categories = categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), categories = categories)\n",
    "newsgroups_train.data\n",
    "newsgroups_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6af6d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(newsgroups_train.data, newsgroups_train.target, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf14c6b0",
   "metadata": {},
   "source": [
    "### Define a pipeline combining a text feature extractor with a simple classifier\n",
    "## Logistic Regression with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cad0ad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_LR = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f28b63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_Score 0.7170542635658915\n"
     ]
    }
   ],
   "source": [
    "pipeline_LR.fit(X_train,y_train)\n",
    "y_pred=pipeline_LR.predict(X_test)\n",
    "acc_lr_cv = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy_Score\", acc_lr_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17515d9",
   "metadata": {},
   "source": [
    "## Multinomial Naïve Bayes with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deadc065",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_Multi = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf',  MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "647f2fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_Score 0.5581395348837209\n"
     ]
    }
   ],
   "source": [
    "pipeline_Multi.fit(X_train,y_train)\n",
    "y_pred=pipeline_Multi.predict(X_test)\n",
    "acc_Multi_cv = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy_Score\", acc_Multi_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97ec52b",
   "metadata": {},
   "source": [
    "##  Support Vector Machines with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0adb8a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_SVC = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf',  SVC()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c382fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_Score 0.7248062015503876\n"
     ]
    }
   ],
   "source": [
    "pipeline_SVC.fit(X_train,y_train)\n",
    "y_pred=pipeline_SVC.predict(X_test)\n",
    "acc_SVC_cv = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy_Score\", acc_SVC_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063370db",
   "metadata": {},
   "source": [
    "##  Decision Trees with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f866d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_Decision = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', DecisionTreeClassifier()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b98dd199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_Score 0.6046511627906976\n"
     ]
    }
   ],
   "source": [
    "pipeline_Decision.fit(X_train,y_train)\n",
    "y_pred=pipeline_Decision.predict(X_test)\n",
    "acc_Decision_cv = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy_Score\", acc_Decision_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabdcb3e",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de94dc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading word2vec\n"
     ]
    }
   ],
   "source": [
    "model = KeyedVectors.load_word2vec_format('C:/Users/chail/googlenews/GoogleNews-vectors-negative300.bin.gz',binary=True)\n",
    "print('done loading word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a749b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_feats(list_of_lists):\n",
    "    DIMENSION = 300\n",
    "    zero_vector = np.zeros(DIMENSION)\n",
    "    feats = []\n",
    "    for tokens in list_of_lists:\n",
    "        feat_for_this = np.zeros(DIMENSION)\n",
    "        count_for_this = 0\n",
    "        for token in tokens :\n",
    "            if token in model:\n",
    "                feat_for_this += model[token]\n",
    "                count_for_this += 1\n",
    "        feats.append(feat_for_this/count_for_this)\n",
    "    return np.array(feats, dtype=float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc607206",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chail\\AppData\\Local\\Temp/ipykernel_21960/859249328.py:12: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feats.append(feat_for_this/count_for_this)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599\n",
      "258\n"
     ]
    }
   ],
   "source": [
    "X_train_feats = embedding_feats(X_train)\n",
    "X_test_feats = embedding_feats(X_test)\n",
    "print(len(X_train_feats))\n",
    "print(len(X_test_feats))\n",
    "#X_train_feats\n",
    "#X_test_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c33baf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputer.fit(X_train_feats)\n",
    "X_train_feats_scale = imputer.transform(X_train_feats)\n",
    "X_test_feats_scale = imputer.transform(X_test_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54db00fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_feats_scale_minmax = scaler.fit_transform(X_train_feats_scale)\n",
    "X_test_feats_scale_minmax = scaler.transform(X_test_feats_scale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5d2c78",
   "metadata": {},
   "source": [
    "## MultinomialNB with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ae42d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_Score_w2v_mnb 0.5310077519379846\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_feats_scale_minmax, y_train)\n",
    "y_pred = clf.predict(X_test_feats_scale_minmax)\n",
    "acc_mnb_w2v = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy_Score_w2v_mnb\", acc_mnb_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a3601a",
   "metadata": {},
   "source": [
    "## Logistic Regression with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99a674b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_Score_w2v_lr 0.5581395348837209\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_feats_scale, y_train)\n",
    "y_pred = lr.predict(X_test_feats_scale)\n",
    "acc_lr_w2v = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy_Score_w2v_lr\", acc_lr_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9cae3b",
   "metadata": {},
   "source": [
    "## SVM with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7b0cefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_Score_w2v_svm 0.5465116279069767\n"
     ]
    }
   ],
   "source": [
    "#count vectoizer and svm\n",
    "svm_model = SVC(kernel='linear', C=1)\n",
    "svm_model.fit(X_train_feats_scale, y_train)\n",
    "y_pred = svm_model.predict(X_test_feats_scale)\n",
    "acc_svm_w2v = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy_Score_w2v_svm\", acc_svm_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777d6843",
   "metadata": {},
   "source": [
    "## Decision Tree with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73e3bad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_Score_w2v 0.5348837209302325\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train_feats_scale, y_train)\n",
    "y_pred = dt.predict(X_test_feats_scale)\n",
    "acc_dt_w2v = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy_Score_w2v\", acc_dt_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf9c9ab",
   "metadata": {},
   "source": [
    "# Table for best combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "031a59b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy Score using Word2Vec</th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy Score using Count Vectorizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.531008</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.558140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.717054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.724806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.604651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model  Accuracy Score using Word2Vec               Model  \\\n",
       "0       MultinomialNB                       0.531008       MultinomialNB   \n",
       "1  LogisticRegression                       0.558140  LogisticRegression   \n",
       "2                 SVM                       0.546512                 SVM   \n",
       "3        DecisionTree                       0.534884        DecisionTree   \n",
       "\n",
       "   Accuracy Score using Count Vectorizer  \n",
       "0                               0.558140  \n",
       "1                               0.717054  \n",
       "2                               0.724806  \n",
       "3                               0.604651  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_w2v = {\n",
    "    'Model': ['MultinomialNB', 'LogisticRegression', 'SVM', 'DecisionTree'],\n",
    "    'Accuracy Score using Word2Vec': [acc_mnb_w2v, acc_lr_w2v, acc_svm_w2v, acc_dt_w2v]\n",
    "}\n",
    "data_cv = {'Model': ['MultinomialNB', 'LogisticRegression', 'SVM', 'DecisionTree'],\n",
    "    'Accuracy Score using Count Vectorizer': [acc_Multi_cv, acc_lr_cv, acc_SVC_cv, acc_Decision_cv]\n",
    "}\n",
    "\n",
    "df = pd.concat([pd.DataFrame(data_w2v), pd.DataFrame(data_cv)], axis=1)\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
